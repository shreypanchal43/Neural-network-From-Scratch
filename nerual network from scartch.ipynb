{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"fashion-mnist_train.csv\")\n",
    "test_data = pd.read_csv(\"fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img=train_data.iloc[0:1,1:]\n",
    "img=np.array(img)\n",
    "\n",
    "img=np.reshape(img,(28,28,1))\n",
    "plt.imshow(img,cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_data.drop([\"label\"],axis=1), train_data[\"label\"] \n",
    "x_test, y_test = test_data.drop([\"label\"],axis=1), test_data[\"label\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train / 255).astype(\"float32\").to_numpy()\n",
    "x_test = (x_test / 255).astype(\"float32\").to_numpy()\n",
    "\n",
    "y_train = pd.get_dummies(train_data['label'],drop_first=True).to_numpy()\n",
    "y_test = pd.get_dummies(test_data['label'],drop_first=True).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, sizes, epochs, learning_rate):\n",
    "            self.sizes = sizes\n",
    "            self.epochs = epochs\n",
    "            self.learning_rate = learning_rate\n",
    "            self.l1_m = 0\n",
    "            self.l2_m = 0\n",
    "            self.l3_m = 0\n",
    "            self.l1_v = 0\n",
    "            self.l2_v = 0\n",
    "            self.l3_v = 0\n",
    "            self.b1 = 0.9\n",
    "            self.b2 = 0.99\n",
    "            self.epsilon = 10e-8\n",
    "            input_layer=self.sizes[0]\n",
    "            hidden_1=self.sizes[1]\n",
    "            hidden_2=self.sizes[2]\n",
    "            output_layer=self.sizes[3]\n",
    "            self.w0 = np.random.normal(0.0, 0.1, (input_layer, hidden_1))\n",
    "            self.w1 = np.random.normal(0.0, 0.1, (hidden_1, hidden_2))\n",
    "            self.w2 = np.random.normal(0.0, 0.1, (hidden_2, output_layer))            \n",
    "    \n",
    "    def run(self, x_train, y_train, iteration, optimizer=\"adam\"):\n",
    "        # forward pass\n",
    "        self.l0 = x_train\n",
    "        self.l1 = self.sigmoid(np.dot(self.l0, self.w0))\n",
    "        self.l2 = self.sigmoid(np.dot(self.l1, self.w1))\n",
    "        self.l3 = self.softmax(np.dot(self.l2, self.w2))\n",
    "        \n",
    "        # backward pass\n",
    "        self.l3_error = self.l3 - y_train\n",
    "        self.l3_delta = self.l3_error * self.sigmoid(self.l3, derivative=True)\n",
    "        self.l2_error = self.l3_delta.dot(self.w2.T)\n",
    "        self.l2_delta = self.l2_error * self.sigmoid(self.l2, derivative=True)\n",
    "        self.l1_error = self.l2_delta.dot(self.w1.T)\n",
    "        self.l1_delta = self.l1_error * self.sigmoid(self.l1, derivative=True)\n",
    "        \n",
    "        change_w = {}\n",
    "        \n",
    "        if optimizer == \"SGD\":\n",
    "            self.w2 -= (self.learning_rate * self.l2.T.dot(self.l3_delta))\n",
    "            self.w1 -= (self.learning_rate * self.l1.T.dot(self.l2_delta))\n",
    "            self.w0 -= (self.learning_rate * self.l0.T.dot(self.l1_delta))\n",
    "            change_w[\"w2\"] = self.w2\n",
    "            change_w[\"w1\"] = self.w1\n",
    "            change_w[\"w0\"] = self.w0\n",
    "            return change_w\n",
    "        \n",
    "        if optimizer == \"adam\":\n",
    "            #gradient calculation\n",
    "            g2 = self.l2.T.dot(self.l3_delta)\n",
    "            g1 = self.l1.T.dot(self.l2_delta)\n",
    "            g0 = self.l0.T.dot(self.l1_delta)\n",
    "\n",
    "            # calculating momentum\n",
    "            self.l3_m = (self.l3_m*self.b1) + ((1 - self.b1)*g2)\n",
    "            self.l2_m = (self.l2_m*self.b1) + ((1 - self.b1)*g1)\n",
    "            self.l1_m = (self.l1_m*self.b1) + ((1 - self.b1)*g0)\n",
    "\n",
    "            self.l3_v = (self.l3_v*self.b2) + ((1 - self.b2)*(g2**2))\n",
    "            self.l2_v = (self.l2_v*self.b2) + ((1 - self.b2)*(g1**2))\n",
    "            self.l1_v = (self.l1_v*self.b2) + ((1 - self.b2)*(g0**2))\n",
    "\n",
    "            # calculating momentum hats\n",
    "            l3_mhat = self.l3_m / (1 - (self.b1**iteration))\n",
    "            l2_mhat = self.l2_m / (1 - (self.b1**iteration))\n",
    "            l1_mhat = self.l1_m / (1 - (self.b1**iteration))\n",
    "\n",
    "            l3_vhat = self.l3_v / (1 - (self.b2**iteration))\n",
    "            l2_vhat = self.l2_v / (1 - (self.b2**iteration))\n",
    "            l1_vhat = self.l1_v / (1 - (self.b2**iteration))\n",
    "\n",
    "            # updating weights\n",
    "            w3_change = l3_mhat / (np.sqrt(l3_vhat) + self.epsilon)\n",
    "            w2_change = l2_mhat / (np.sqrt(l2_vhat) + self.epsilon)\n",
    "            w1_change = l1_mhat / (np.sqrt(l1_vhat) + self.epsilon)\n",
    "            \n",
    "            self.w2 = self.w2 - (self.learning_rate * w3_change)\n",
    "            self.w1 = self.w1 - (self.learning_rate * w2_change)\n",
    "            self.w0 = self.w0 - (self.learning_rate * w1_change)\n",
    "            change_w[\"w2\"] = self.w2\n",
    "            change_w[\"w1\"] = self.w1\n",
    "            change_w[\"w0\"] = self.w0\n",
    "            return change_w\n",
    "        \n",
    "        if optimizer == \"nadam\":\n",
    "            # gradient calculation\n",
    "            g2 = self.l2.T.dot(self.l3_delta)\n",
    "            g1 = self.l1.T.dot(self.l2_delta)\n",
    "            g0 = self.l0.T.dot(self.l1_delta)\n",
    "            \n",
    "            # calculating momentum\n",
    "            self.l3_m = (self.l3_m*self.b1) + ((1 - self.b1)*g2)\n",
    "            self.l2_m = (self.l2_m*self.b1) + ((1 - self.b1)*g1)\n",
    "            self.l1_m = (self.l1_m*self.b1) + ((1 - self.b1)*g0)\n",
    "\n",
    "            self.l3_v = (self.l3_v*self.b2) + ((1 - self.b2)*(g2**2))\n",
    "            self.l2_v = (self.l2_v*self.b2) + ((1 - self.b2)*(g1**2))\n",
    "            self.l1_v = (self.l1_v*self.b2) + ((1 - self.b2)*(g0**2))            \n",
    "\n",
    "            # calculating momentum hats\n",
    "            l3_mhat = (self.l3_m / (1 - (self.b1**iteration))) + ((1 - self.b1) * g2 / (1 - (self.b1**iteration)))\n",
    "            l2_mhat = (self.l2_m / (1 - (self.b1**iteration))) + ((1 - self.b1) * g1 / (1 - (self.b1**iteration)))\n",
    "            l1_mhat = (self.l1_m / (1 - (self.b1**iteration))) + ((1 - self.b1) * g0 / (1 - (self.b1**iteration)))\n",
    "\n",
    "            l3_vhat = self.l3_v / (1 - (self.b2**iteration))\n",
    "            l2_vhat = self.l2_v / (1 - (self.b2**iteration))\n",
    "            l1_vhat = self.l1_v / (1 - (self.b2**iteration))\n",
    "\n",
    "            # updating weights\n",
    "            w3_change = l3_mhat / (np.sqrt(l3_vhat) + self.epsilon)\n",
    "            w2_change = l2_mhat / (np.sqrt(l2_vhat) + self.epsilon)\n",
    "            w1_change = l1_mhat / (np.sqrt(l1_vhat) + self.epsilon)\n",
    "            \n",
    "            self.w2 = self.w2 - (self.learning_rate * w3_change)\n",
    "            self.w1 = self.w1 - (self.learning_rate * w2_change)\n",
    "            self.w0 = self.w0 - (self.learning_rate * w1_change)\n",
    "            change_w[\"w2\"] = self.w2\n",
    "            change_w[\"w1\"] = self.w1\n",
    "            change_w[\"w0\"] = self.w0\n",
    "            return change_w\n",
    "        \n",
    "        if optimizer == \"rms\":\n",
    "            # gradient calculation\n",
    "            g2 = self.l2.T.dot(self.l3_delta)\n",
    "            g1 = self.l1.T.dot(self.l2_delta)\n",
    "            g0 = self.l0.T.dot(self.l1_delta)\n",
    " \n",
    "            # calculating momentum\n",
    "            self.l3_m = (self.l3_m*self.b1) + ((1 - self.b1)*(g2 ** 2))\n",
    "            self.l2_m = (self.l2_m*self.b1) + ((1 - self.b1)*(g1 ** 2))\n",
    "            self.l1_m = (self.l1_m*self.b1) + ((1 - self.b1)*(g0 ** 2))\n",
    "            \n",
    "            # updating weights\n",
    "            w3_change = g2 / (np.sqrt(self.l3_m + self.epsilon))\n",
    "            w2_change = g1 / (np.sqrt(self.l2_m + self.epsilon))\n",
    "            w1_change = g0 / (np.sqrt(self.l1_m + self.epsilon))\n",
    "            \n",
    "            self.w2 = self.w2 - (self.learning_rate * w3_change)\n",
    "            self.w1 = self.w1 - (self.learning_rate * w2_change)\n",
    "            self.w0 = self.w0 - (self.learning_rate * w1_change)\n",
    "            change_w[\"w2\"] = self.w2\n",
    "            change_w[\"w1\"] = self.w1\n",
    "            change_w[\"w0\"] = self.w0\n",
    "            return change_w\n",
    "        \n",
    "        if optimizer == \"gd_momentum\":\n",
    "            # gradient calculation\n",
    "            g2 = self.l2.T.dot(self.l3_delta)\n",
    "            g1 = self.l1.T.dot(self.l2_delta)\n",
    "            g0 = self.l0.T.dot(self.l1_delta)\n",
    "            \n",
    "            # calculating momentum\n",
    "            self.l3_m = (self.l3_m*self.b1) + (self.learning_rate*g2)\n",
    "            self.l2_m = (self.l2_m*self.b1) + (self.learning_rate*g1)\n",
    "            self.l1_m = (self.l1_m*self.b1) + (self.learning_rate*g0)\n",
    "            \n",
    "            # updating weights\n",
    "            self.w2 = self.w2 - self.l3_m\n",
    "            self.w1 = self.w1 - self.l2_m\n",
    "            self.w0 = self.w0 - self.l1_m\n",
    "            change_w[\"w2\"] = self.w2\n",
    "            change_w[\"w1\"] = self.w1\n",
    "            change_w[\"w0\"] = self.w0\n",
    "            return change_w\n",
    "        \n",
    "        if optimizer == \"gd_nestrov\":\n",
    "            # gradient calculation\n",
    "            g2 = self.l2.T.dot(self.l3_delta)\n",
    "            g1 = self.l1.T.dot(self.l2_delta)\n",
    "            g0 = self.l0.T.dot(self.l1_delta)\n",
    "            \n",
    "            # calculating nestrov momentum\n",
    "            self.l3_m = (self.l3_m*self.b1) - self.learning_rate*g2\n",
    "            self.l2_m = (self.l2_m*self.b1) - self.learning_rate*g1\n",
    "            self.l1_m = (self.l1_m*self.b1) - self.learning_rate*g0\n",
    "            \n",
    "            # updating weights\n",
    "            self.w2 = self.w2 + self.l3_m\n",
    "            self.w1 = self.w1 + self.l2_m\n",
    "            self.w0 = self.w0 + self.l1_m\n",
    "            change_w[\"w2\"] = self.w2\n",
    "            change_w[\"w1\"] = self.w1\n",
    "            change_w[\"w0\"] = self.w0\n",
    "            return change_w           \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def forward_pass(self, x_train):\n",
    "            # input layer activations becomes sample\n",
    "            l0 = x_train #60000, 784\n",
    "            # input layer to hidden layer 1\n",
    "            l0_d = np.dot( l0, self.w0) #\n",
    "            out1 = self.sigmoid(l0_d)\n",
    "            # hidden layer 1 to hidden layer 2\n",
    "            l1_d = np.dot(out1, self.w1)\n",
    "            out2 = self.sigmoid(l1_d)\n",
    "            # hidden layer 2 to output layer\n",
    "            l2_d = np.dot(out2, self.w2)\n",
    "            out3 = self.softmax(l2_d)\n",
    "            return out3\n",
    "    \n",
    "    # Same as forward pass\n",
    "    def get_l3(self, x_train):\n",
    "        l0 = x_train\n",
    "        l1 = self.sigmoid(np.dot(l0, self.w0))\n",
    "        l2 = self.sigmoid(np.dot(l1, self.w1))\n",
    "        l3 = self.softmax(np.dot(l2, self.w2))\n",
    "        return l3\n",
    "    \n",
    "    def MSEloss(self, y, Y):\n",
    "        return np.mean((y-Y)**2)\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x, derivative=False):\n",
    "        exps = np.exp(x - x.max())\n",
    "        if derivative:\n",
    "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "   \n",
    "    def predict(self, x, y):\n",
    "        predictions = []\n",
    "        output = self.get_l3(x)\n",
    "\n",
    "        for i,j in zip(output,y):\n",
    "            y_pred = np.argmax(i)\n",
    "            predictions.append(y_pred==np.argmax(j))\n",
    "        print(np.mean(predictions)) \n",
    "    \n",
    "    def compute_accuracy(self, x_val, y_val):\n",
    "        predictions = []\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.forward_pass(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(y))\n",
    "        return np.mean(predictions)\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, optimizer=\"adam\"):\n",
    "        start_time = time.time()\n",
    "        if optimizer == \"adam\":\n",
    "            print(\"Using adam optimizer\")\n",
    "            for iteration in range(self.epochs):\n",
    "                change_w = self.run(x_train, y_train, iteration+1, optimizer=\"adam\")\n",
    "                l3 = self.get_l3(x_train)\n",
    "                l = self.MSEloss(l3, y_train)\n",
    "                print('Epoch: {0}, Time Spent: {1:.2f}s, Loss: {2}'.format(iteration+1, time.time() - start_time, l ))\n",
    "            self.predict(x_test, y_test)\n",
    "        if optimizer == \"SGD\":\n",
    "            print(\"Using SGD optimizer\")\n",
    "            for iteration in range(self.epochs):\n",
    "                change_w = self.run(x_train,y_train,iteration+1,optimizer=\"SGD\")\n",
    "                l3 = self.get_l3(x_train)\n",
    "                l = self.MSEloss(l3, y_train)\n",
    "                print('Epoch: {0}, Time Spent: {1:.2f}s, Loss: {2}'.format(iteration+1, time.time() - start_time, l ))\n",
    "            self.predict(x_test, y_test)\n",
    "        if optimizer == \"nadam\":\n",
    "            print(\"Using nadam optimizer\")\n",
    "            for iteration in range(self.epochs):\n",
    "                change_w = self.run(x_train, y_train, iteration+1, optimizer=\"nadam\")\n",
    "                l3 = self.get_l3(x_train)\n",
    "                l = self.MSEloss(l3, y_train)\n",
    "                print('Epoch: {0}, Time Spent: {1:.2f}s, Loss: {2}'.format(iteration+1, time.time() - start_time, l ))\n",
    "            self.predict(x_test, y_test)\n",
    "        if optimizer == \"rms\":\n",
    "            print(\"Using rms optimizer\")\n",
    "            for iteration in range(self.epochs):\n",
    "                change_w = self.run(x_train, y_train, iteration+1, optimizer=\"rms\")\n",
    "                l3 = self.get_l3(x_train)\n",
    "                l = self.MSEloss(l3, y_train)\n",
    "                print('Epoch: {0}, Time Spent: {1:.2f}s, Loss: {2}'.format(iteration+1, time.time() - start_time, l ))\n",
    "            self.predict(x_test, y_test)\n",
    "        if optimizer == \"gd_momentum\":\n",
    "            print(\"Using GDmomentum optimizer\")\n",
    "            for iteration in range(self.epochs):\n",
    "                change_w = self.run(x_train, y_train, iteration+1, optimizer=\"gd_momentum\")\n",
    "                l3 = self.get_l3(x_train)\n",
    "                l = self.MSEloss(l3, y_train)\n",
    "                print('Epoch: {0}, Time Spent: {1:.2f}s, Loss: {2}'.format(iteration+1, time.time() - start_time, l ))\n",
    "            self.predict(x_test, y_test)\n",
    "        if optimizer == \"gd_nestrov\":\n",
    "            print(\"Using GDnestrov optimizer\")\n",
    "            for iteration in range(self.epochs):\n",
    "                change_w = self.run(x_train, y_train, iteration+1, optimizer=\"gd_nestrov\")\n",
    "                l3 = self.get_l3(x_train)\n",
    "                l = self.MSEloss(l3, y_train)\n",
    "                print('Epoch: {0}, Time Spent: {1:.2f}s, Loss: {2}'.format(iteration+1, time.time() - start_time, l ))\n",
    "            self.predict(x_test, y_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = AdamOptimizer(sizes=[x_train.shape[1], 128, 128, 9], epochs=20, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using adam optimizer\n",
      "Epoch: 1, Time Spent: 1.69s, Loss: 0.09999660456834393\n",
      "Epoch: 2, Time Spent: 3.45s, Loss: 0.09999656041022126\n",
      "Epoch: 3, Time Spent: 5.27s, Loss: 0.09999651946830336\n",
      "Epoch: 4, Time Spent: 7.01s, Loss: 0.09999648255907984\n",
      "Epoch: 5, Time Spent: 8.74s, Loss: 0.09999645018638431\n",
      "Epoch: 6, Time Spent: 10.51s, Loss: 0.0999964225593269\n",
      "Epoch: 7, Time Spent: 12.16s, Loss: 0.09999639963643085\n",
      "Epoch: 8, Time Spent: 13.93s, Loss: 0.0999963811789091\n",
      "Epoch: 9, Time Spent: 15.70s, Loss: 0.09999636681427536\n",
      "Epoch: 10, Time Spent: 17.43s, Loss: 0.09999635610294871\n",
      "Epoch: 11, Time Spent: 19.32s, Loss: 0.09999634858969511\n",
      "Epoch: 12, Time Spent: 21.03s, Loss: 0.09999634383365809\n",
      "Epoch: 13, Time Spent: 22.83s, Loss: 0.09999634142280657\n",
      "Epoch: 14, Time Spent: 24.72s, Loss: 0.09999634097841265\n",
      "Epoch: 15, Time Spent: 26.77s, Loss: 0.09999634215442361\n",
      "Epoch: 16, Time Spent: 28.73s, Loss: 0.09999634463546009\n",
      "Epoch: 17, Time Spent: 30.67s, Loss: 0.09999634813451033\n",
      "Epoch: 18, Time Spent: 32.91s, Loss: 0.09999635239326078\n",
      "Epoch: 19, Time Spent: 34.80s, Loss: 0.09999635718584093\n",
      "Epoch: 20, Time Spent: 36.68s, Loss: 0.09999636232253319\n",
      "(10000, 9)\n",
      "0.4567\n"
     ]
    }
   ],
   "source": [
    "dnn.train(x_train, y_train, x_test, y_test, optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "pred = dnn.predict(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
